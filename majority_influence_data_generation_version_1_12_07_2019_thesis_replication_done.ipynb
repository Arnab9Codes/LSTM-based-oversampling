{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "majority_influence_data_generation_version_1-12-07-2019-thesis-replication-done.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arnab9Codes/LSTM-based-oversampling/blob/master/majority_influence_data_generation_version_1_12_07_2019_thesis_replication_done.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ozUI48dCoQg",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyvTb215uRv0",
        "colab_type": "code",
        "outputId": "507c6c10-fbae-4681-a859-8c73e705025e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')#,force_remount=True"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3jdjlf020I5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aP_42P_8pKz5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import f1_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import auc\n",
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import time\n",
        "from datetime import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rq3dbsxopK0Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def preprocessing(dat_data,validation,seed):\n",
        "    \n",
        "    cur_dir=os.getcwd()\n",
        "    df=pd.read_csv('/content/drive/My Drive/codes_first_try/data/'+dat_data)#reading the .dat file or .csv file\n",
        "    # the folder location might change and come changes in the above line will be required for getting to the folder of the dataset\n",
        "    \n",
        "    #df_min=df[df['Outcome']==' negative']#getting negative samples.... \n",
        "    #df_majority=df[df['Outcome']==' positive']#getting positive samples...\n",
        "    \n",
        "    \n",
        "    #separating majority and minority data\n",
        "    #df_min.to_csv('ecoli-0_vs_1_minority.csv',index=False) #converting negative samples dataframe to a csv file\n",
        "    #df_majority.to_csv('ecoli_0_vs_1_majority.csv',index=False) #converting positive samples dataframe to a csv file\n",
        "    df_val=df.values\n",
        "    \n",
        "    data=np.array(df.values)\n",
        "\n",
        "    pos=data.shape[1]-1 #getting target column\n",
        "\n",
        "    for i in range(data.shape[0]):\n",
        "        if data[i][pos]==' negative' or data[i][pos]=='negative':\n",
        "            data[i][pos]=0\n",
        "        else:\n",
        "            data[i][pos]=1\n",
        "\n",
        "\n",
        "    #min_data=np.array(df_min) #getting minority samples...\n",
        "    #maj_data=np.array(df_majority) #getting majority samples...\n",
        "    \n",
        "    validation=validation # set in function call\n",
        "    seed=seed #set in function call\n",
        "    \n",
        "    X=data[:,:pos].astype(float)# getting the feature values\n",
        "    Y=data[:,pos].astype(int)# getting prediction\n",
        "    \n",
        "    X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=validation,random_state=seed) #making a train test split\n",
        "    \n",
        "    Y_train=Y_train.reshape((Y_train.shape[0],1)) #reshaping for latter concatenation purpose\n",
        "    \n",
        "    train_Data=np.concatenate((X_train,Y_train),axis=1) #concatenating to produce full training data\n",
        "    \n",
        "    Y_test=Y_test.reshape((Y_test.shape[0],1)) # reshaping for latter concatenation purpose\n",
        "    \n",
        "    test_Data=np.concatenate((X_test,Y_test),axis=1) #concatenating to produce full test data\n",
        "    \n",
        "    #coverting numpy array to dataframe\n",
        "    train_Data=pd.DataFrame(train_Data)\n",
        "    test_Data=pd.DataFrame(test_Data)\n",
        "    \n",
        "    #saving dataframe to csv files\n",
        "    #train_Data.to_csv(dat_data+'-seed-'+str(seed)+'-train_Data.csv',index=False)\n",
        "    #test_Data.to_csv(dat_data+'-seed-'+str(seed)+'-test_Data.csv',index=False) \n",
        "    \n",
        "    return train_Data,test_Data\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peWioOZqpK0b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function for making a series of observations predicting a target\n",
        "def create_dataset(dataset_min,dataset_majority,look_back=1,majority_influence=0):\n",
        "    \n",
        "    datax,datay=[],[]\n",
        "\n",
        "\n",
        "    \n",
        "    for i in range(len(dataset_min)-look_back-1):\n",
        "        a=dataset_min[i:(i+look_back),:]\n",
        "        datax.append(a)\n",
        "        datay.append(dataset_min[i+look_back,:])\n",
        "        \n",
        "    datax=np.array(datax)\n",
        "    datay=np.array(datay)\n",
        "\n",
        "    for i in range(datax.shape[0]):\n",
        "      list_of_majority_index=np.random.randint(0,dataset_majority.shape[0],majority_influence)\n",
        "      for j in list_of_majority_index:\n",
        "        pick_index=np.random.randint(0,look_back,1)\n",
        "        datax[i][pick_index]=dataset_majority[j]\n",
        "\n",
        "    return np.array(datax),np.array(datay)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgkxoTGzpK0y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function for sample generation using LSTM based technique\n",
        "\n",
        "def LSTM_based_oversampling(dat_data,train_data,validation,seed,series_length):\n",
        "    \n",
        "    #df=pd.read_csv('train_Data.csv')# reading only train data separated in data preprocessing \n",
        "    df=train_data\n",
        "    min_class=0 #intiazing\n",
        "    maj_class=1 #initializing\n",
        "    \n",
        "    #print(df.shape)\n",
        "    \n",
        "    pos=df.shape[1]-1 #getting target column\n",
        "    \n",
        "    #print(pos)\n",
        "    \n",
        "    zeros=0\n",
        "    ones=0\n",
        "    \n",
        "    df_val=df.values\n",
        "    \n",
        "    col=df.columns[pos]\n",
        "    \n",
        "    for i in range(0,df_val.shape[0],1):\n",
        "        if(df_val[i][pos]==0):\n",
        "            zeros=zeros+1\n",
        "        else:\n",
        "            ones=ones+1\n",
        "    \n",
        "    if zeros<=ones:\n",
        "        min_class=0\n",
        "        maj_class=1\n",
        "    else:\n",
        "        min_class=1\n",
        "        maj_class=0\n",
        "    \n",
        "    #print('zeros: ',zeros,' ones: ',ones)\n",
        "    #getting only minority samples from the train data\n",
        "    df_min=df[df[col]==min_class]\n",
        "    #saving it as a csv file\n",
        "    \n",
        "    #print('df_min: ',df_min.shape)\n",
        "    #df_min.to_csv('minority_train.csv',index=False) #---------------------------------------\n",
        "    #df_min=pd.read_csv('minority_train.csv')\n",
        "    \n",
        "    #print(df_min.shape)\n",
        "    \n",
        "    df_majority=df[df[col]==maj_class]\n",
        "    #print('df_maj: ',df_majority.shape)\n",
        "    #df_majority.to_csv('majority_train.csv',index=False)\n",
        "    df_minor=np.array(df_min)\n",
        "    df_majority=np.array(df_majority)\n",
        "    \n",
        "    scaler=MinMaxScaler(feature_range=(0,1))\n",
        "    df_minor=scaler.fit_transform(df_min)\n",
        "    df_majority=scaler.fit_transform(df_majority)\n",
        "\n",
        "    x,y=create_dataset(df_minor,df_majority,series_length,1)#converting into a series dataset for prediction\n",
        "    \n",
        "    Xtrain,xtest,Ytrain,ytest=train_test_split(x,y,test_size=validation,random_state=seed)# set to 40% test set here for generating 40% more minority data\n",
        "    \n",
        "    \n",
        "    # Code for LSTM based neural network\n",
        "    model=Sequential()\n",
        "    model.add(LSTM(20,input_shape=(Xtrain.shape[1],Xtrain.shape[2])))#5\n",
        "    model.add(Dense(Xtrain.shape[2]))\n",
        "    \n",
        "    model.compile(loss='mse',optimizer='adam')\n",
        "    \n",
        "    history=model.fit(Xtrain,Ytrain,epochs=500,verbose=0)\n",
        "    \n",
        "    prediction=model.predict(xtest)#making prediction, this prediction is the newly generated minority data\n",
        "    \n",
        "    prediction=scaler.inverse_transform(prediction)#converting back to original scale\n",
        "    new_data=pd.DataFrame(prediction) #converting the numpy array into a dataframe for saving it into a csv file\n",
        "    \n",
        "    synthetic_data=new_data\n",
        "    \n",
        "    synthetic_data[col]=min_class\n",
        "    \n",
        "    #new_data.to_csv(dat_data+'-seed-'+str(seed)+'_LSTM_generated_data.csv',index=False)\n",
        "    \n",
        "    return synthetic_data\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4NMd2lEsd4O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def svc_param_selection(X,Y,folds,seed):\n",
        "    \n",
        "  Cs=[0.0001,0.001, 0.01, 0.1, 1, 10] #Cs should be increased\n",
        "\n",
        "  gammas=[0.0001,0.001, 0.01, 0.1, 1, 10]#gammas should be increased\n",
        "    \n",
        "  param_grid={'C':Cs,'gamma':gammas}\n",
        "    \n",
        "  model=GridSearchCV(SVC(kernel='rbf',probability=True,random_state=seed),param_grid,cv=folds)#performing grid_seach with probablity=True\n",
        "    \n",
        "  model.fit(X,Y)#fitting the data(training the model on the data)\n",
        "    \n",
        "    #finally returning the best estimator\n",
        "  return model.best_estimator_\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHcBWzfkpK0-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def svm_classification(train,test,synthetic_data,seed):\n",
        "    \n",
        "    extra_data=synthetic_data#pd.read_csv('LSTM_generated_data.csv')# loading extra genereated data genereated by LSTM\n",
        "    original_data=train#pd.read_csv('train_Data.csv')#original data\n",
        "    \n",
        "    pos=extra_data.shape[1]-1   # will be same on code: pos=original_data.shape[1]-1\n",
        "    \n",
        "    extra_val=extra_data.values\n",
        "    \n",
        "    if extra_val[0][pos]<0.5:\n",
        "        for i in range(0,extra_data.shape[0],1):\n",
        "            extra_val[i][pos]=0\n",
        "    else:\n",
        "        for i in range(0,extra_data.shape[0],1):\n",
        "            extra_val[i][pos]=1\n",
        "    \n",
        "    values=original_data.values\n",
        "    \n",
        "    X=values[:,:pos].astype(float)#getting training dat from original csv file\n",
        "    Y=values[:,pos].astype(int)#getting target from original training csv file\n",
        "    \n",
        "    #now doing the same for extra data generated by lstm_genereated_sample function\n",
        "    #extra_val=extra_data.values\n",
        "    \n",
        "    extra_X=extra_val[:,:pos].astype(float)\n",
        "    extra_Y=extra_val[:,pos].astype(int)\n",
        "    \n",
        "    #now concatenating values\n",
        "    extrain=np.concatenate((X,extra_X),axis=0)\n",
        "    eytrain=np.concatenate((Y,extra_Y),axis=0)\n",
        "    \n",
        "    #reading test data\n",
        "    test_data=test#pd.read_csv('test_data.csv')\n",
        "    test_val=test_data.values\n",
        "    \n",
        "    test_X=test_val[:,:pos].astype(float)\n",
        "    test_Y=test_val[:,pos].astype(int)\n",
        "    \n",
        "    # for smote oversampling\n",
        "    sm=SMOTE(random_state=seed)\n",
        "    x_train_smote,y_train_smote=sm.fit_sample(X,Y)#using smote for balnacing it completely\n",
        "\n",
        "    #kernel-rbf, svm using only original data, cross_validation=5(vary it if needed)\n",
        "    #print(\"without exta: \")\n",
        "    rbf_svc=svc_param_selection(X,Y,5,seed)\n",
        "\n",
        "    # secondly rbf_svc_extra for training on original+LSTM_genearated_data (same criterion as before)\n",
        "    #print('with extra: ')\n",
        "    rbf_svc_extra = svc_param_selection(extrain,eytrain,5,seed)\n",
        "    \n",
        "    # now performing svm on smote oversamplified data\n",
        "    rbf_svc_smote = svc_param_selection(x_train_smote,y_train_smote,5,seed)\n",
        "\n",
        "    #now fitting the data\n",
        "    rbf_svc.fit(X,Y)\n",
        "    rbf_svc_extra.fit(extrain,eytrain)\n",
        "    rbf_svc_smote.fit(x_train_smote,y_train_smote)\n",
        "    \n",
        "    # making predictions on validation dataset using svc trained only on original training data\n",
        "    prediction_on_real_dataset=rbf_svc.predict_proba(test_X)\n",
        "\n",
        "    #saving probabilities\n",
        "    predictions=rbf_svc.predict(test_X)\n",
        "\n",
        "    # making predictions on validation dataset using svc trained on original training data + LSTM generated data\n",
        "    prediction_on_real_dataset_adding_extra=rbf_svc_extra.predict_proba(test_X)\n",
        "    #saving probabilities\n",
        "    predictions_extra=rbf_svc_extra.predict(test_X)\n",
        "    \n",
        "    #making predictions on balanced data using smote\n",
        "    prediction_on_balanced_dataset_smote=rbf_svc_smote.predict_proba(test_X)\n",
        "    #saving probabilities\n",
        "    predictions_smote=rbf_svc_smote.predict(test_X)\n",
        "\n",
        "    #generating f1 scores\n",
        "    f1_score_without_extra=f1_score(test_Y,predictions)\n",
        "    f1_score_with_extra=f1_score(test_Y,predictions_extra)\n",
        "    f1_score_with_smote=f1_score(test_Y,predictions_smote)\n",
        "    \n",
        "    #print('F1-score without extra: ',f1_score(test_Y,predictions)) # f1_score without extra\n",
        "    #print('F1-score with extra: ',f1_score(test_Y,predictions)) # f1_score with extra\n",
        "    \n",
        "    return  f1_score_without_extra,f1_score_with_extra,f1_score_with_smote\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKKCgJ-qpK1H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed_list=[0,5,9,11,19]\n",
        "dataset_list=[#'ecoli-0_vs_1.dat',\n",
        "              #'pima.dat',\n",
        "              #'glass1.dat',            \n",
        "              #'glass0.dat',\n",
        "              #'iris0.dat',\n",
        "              #'wisconsin.dat',\n",
        "              #'yeast1.dat',\n",
        "              #'yeast3.dat',\n",
        "              'page-blocks0.dat',  \n",
        "              #'segment0.dat'\n",
        "                ]\n",
        "\n",
        "#code_dir=os.getcwd()\n",
        "\n",
        "for i in range(0,len(dataset_list),1):\n",
        "    dat_data=dataset_list[i]\n",
        "  \n",
        "    with open(dat_data+'_experiements_majority-1.txt','a') as file:\n",
        "        file.write('date: ')\n",
        "        file.write(str(datetime.now()))\n",
        "        file.write(\"\\n\\n\")\n",
        "        without_extra=[]\n",
        "        with_extra=[]\n",
        "        smote_balanced=[]\n",
        "\n",
        "        file.write('seed,without_extra,lstm_ov_sampling,balanced(Smote)')\n",
        "        file.write('\\n\\n')\n",
        "        for j in range(0,len(seed_list),1):\n",
        "            \n",
        "            train,test=preprocessing(dat_data,0.30,seed_list[j]) #dat_data,validation,seed\n",
        "            synthetic_data=LSTM_based_oversampling(dat_data,train,0.40,seed_list[j],5) #dat_data,train_data,validation,seed,series_length\n",
        "            f1_score_without_extra,f1_score_with_extra,f1_score_with_smote=svm_classification(train,test,synthetic_data,seed_list[j])\n",
        "            \n",
        "            without_extra.append(f1_score_without_extra)\n",
        "            with_extra.append(f1_score_with_extra)\n",
        "            smote_balanced.append(f1_score_with_smote)\n",
        "\n",
        "            file.write(str(seed_list[j]))\n",
        "            file.write(',')\n",
        "            file.write(str(f1_score_without_extra))\n",
        "            file.write(',')\n",
        "            file.write(str(f1_score_with_extra))\n",
        "            file.write(',')\n",
        "            file.write(str(f1_score_with_smote))\n",
        "            file.write('\\n')\n",
        "        \n",
        "        without_extra=np.array(without_extra)\n",
        "        with_extra=np.array(with_extra)\n",
        "        smote_balanced=np.array(smote_balanced)\n",
        "        \n",
        "        file.write('\\n Mean:\\n\\n')\n",
        "        file.write('Without exra\\t\\tLSTM Oversampling\\t\\tbalanced(SMOTE)\\n\\n')\n",
        "        file.write(str(np.mean(without_extra)))\n",
        "        file.write('\\t')\n",
        "        file.write(str(np.mean(with_extra)))\n",
        "        file.write('\\t')\n",
        "        file.write(str(np.mean(smote_balanced)))\n",
        "        file.write('\\n\\nImprovemnet using LSTM based oversampling: ')\n",
        "        file.write(str((np.mean(with_extra)-np.mean(without_extra))*100))\n",
        "        file.write(' %')\n",
        "        file.write('\\n\\nImprovemnet using SMOTE based oversampling: ')\n",
        "        file.write(str((np.mean(smote_balanced)-np.mean(without_extra))*100))\n",
        "        file.write(' %')\n",
        "        file.write('\\n\\nStandard deviatiion without extra: ')\n",
        "        file.write(str(np.std(without_extra,axis=0)))\n",
        "        file.write('\\nStandard deviation using LSTM: ')\n",
        "        file.write(str(np.std(with_extra,axis=0)))\n",
        "        file.write('\\nStandard deviation using SMOTE balancing: ')\n",
        "        file.write(str(np.std(smote_balanced,axis=0)))\n",
        "        file.write('\\n')\n",
        "       # os.chdir(code_dir)\n",
        "    print(dat_data,' is complete.\\n')\n",
        "    #this statement only for google colab to have a visible output file downloaded but it did not worked for me\n",
        "    #instead search the files section left by clicking >\n",
        "    #files.download(dat_data+'_experiements_.txt')\n",
        "\n",
        "#drive.flush_and_unmount()# this is to save the changes in google drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "si8vgP3dtZng",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}